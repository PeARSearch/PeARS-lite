
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <title>The People's search engine | PeARS</title>
    <link rel="shortcut icon" href="../images/ico/favicon.ico">
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="../images/ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="../images/ico/apple-touch-icon-114-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="../images/ico/apple-touch-icon-72-precomposed.png">
    <link rel="apple-touch-icon-precomposed" href="../images/ico/apple-touch-icon-57-precomposed.png">
    <meta name="generator" content="Bootply"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href="../styles/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Rubik:400,400i,500" rel="stylesheet" />
    <link href="../styles/website.css" rel="stylesheet" />
    <link href="../styles/image.css" rel="stylesheet" />


  </head>
  <body>
  <nav class="navbar navbar-default">
    <div class="container-fluid">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a href="../index.html" class="navbar-left"><img src="../images/pears-only-small.png" height="50px"></a>
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">
        <li><a href="../technology.html">What is PeARS?</a></li>
          <li><a href="../philosophy.html">Philosophy</a></li>
          <li><a href="../posts.html">Blog</a></li>
          <li><a href="../gallery.html">Gallery</a></li>
          <li><a href="../contribute.html">Contribute</a></li>
        <li><a href="../faq.html">F.A.Q.</a></li>
        <li><a href="../downloads.html">Downloads</a></li>
      </ul>
      
    </div><!-- /.navbar-collapse -->
    </div><!-- /.container-fluid -->
    </nav>


  <div class="container-full">
    <div class="row">
    <div class="col-md-8">

    <section class="Layout__Content Article">

  <h1 class="PageTitle">An introduction to the Fruit Fly Algorithm (FFA)</h1>

  <p><a href="https://github.com/PeARSearch/PeARS-fruit-fly">PeARS Fruit Fly</a> is the new PeARS project, under active development. It is aimed at making the PeARS Web indexer smaller, more efficient, and more accurate.</p>

  <p>Inspired by the olfactory system of the species Drosophila Melanogaster (the common fruit fly), the indexer is a shallow, partially connected neural network which transforms the text of Web documents into a compact, searchable representation. Given its tiny size and simplicity, a fruit fly model can be stored and run on any device with great efficiency. </p>

<a title="Madboy74, CC0, via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:Biology_Illustration_Animals_Insects_Drosophila_melanogaster.svg"><img width="256" alt="Biology Illustration Animals Insects Drosophila melanogaster" src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Biology_Illustration_Animals_Insects_Drosophila_melanogaster.svg/256px-Biology_Illustration_Animals_Insects_Drosophila_melanogaster.svg.png"></a>


  
  <h2>The FFA: technical description</h2>

<p>The Fruit Fly Algorithm (FFA), a technique derived from the olfactory system of the species <i>Drosophila melanogaster</i>, implements a kind of local-sensitivity hashing relying on random projections. The usefulness of the biological algorithm for computer science was first noted by <a href="https://science.sciencemag.org/content/358/6364/793.abstract">Dasgupta et al (2016)</a>, who used it for hashing pre-trained document and image vectors. In our project, we aim to take the algorithm one step further and make it learn document representations directly from raw text.</p>


<p>The Fruitfly Algorithm mimics the olfactory system of the fruit fly, which assigns a pattern of binary activations to a particular smell (i.e., a combination of multiple chemicals), using sparse connections between just two neuronal layers. This mechanism allows the fly to 'conceptualise' its environment and to appropriately react to new smells by relating them to previous experiences. </p>

<p>In a computer science context, the FFA can be regarded as a very simple and elegant algorithm, implemented as a small feedforward neural network. We use here below the descriptions of <a href="http://ceur-ws.org/Vol-2481/paper59.pdf">Herbelot and Preissner (2019)</a> and <a href="https://journals.openedition.org/ijcol/609">Herbelot and Preissner (2020)</a>, who ported the original algorithm to a Natural Language Processing setting and made the fly learn word embeddings.</p>

<p>The figure below shows the architecture of the system.</p>

<img src="https://journals.openedition.org/ijcol/docannexe/image/609/img-1-small480.jpg" title="The architecture of the fruit fly algorithm">

<p>The PN (Projection Neurons) layer takes features from raw text documents (see [this wiki page](https://github.com/PeARSearch/PeARS-fruit-fly/wiki/1.1.-Baselines) for more details). It is partially connected to the KC (Kenyon Cell) layer via random projections of a certain size. The *k* most activated Kenyon Cells give out a locality-sensitive hash for each document. That is, the hashes can be used to compute levels of similarity between any pair of documents.</p>




<h2>Getting a Web document vector via the FFA</h2>

<p>We briefly describe below the pipeline for computing a document vector with the FFA.</p>

<p>As features, we use a pre-computed <a href="https://github.com/google/sentencepiece">sentencepiece</a> vocabulary, which chunks a document into wordpieces (i.e. character n-grams). Given a text document <i>d</i>, we first encode <i>d</i> in terms of our wordpiece vocabulary. </p>

<p>We then produce a frequency vector for <i>d</i>. That is, we simply count the number of times that each wordpiece in our vocabulary occurs in the document. In order to avoid giving too much importance to very frequent words like articles or prepositions, we reweigh the vector using the log probabilities computed by sentencepiece on the larger corpus. This 'flattens' the frequency distribution of the words in the document.</p>

<p>We only keep the <i>k</i> most important words for the document, where <i>k</i> is a hyperparameter that can be tuned for the algorithm. </p>

<p>Now, we use the resulting vector as input to the FFA. As explained above, the FFA aggregates input values via random projections linking the input layer to the 'Kenyon Cell' layer of the architecture. In our case, it means that certain combinations of words will more or less strongly activate particular Kenyon cells.</p>

<p>Example: say there is a random projection of size 5 linking input words <i>'cat', 'run', 'think', 'of', 'elaborate'</i> to Kenyon Cell number 36 (KC36), and that our document contains the words <i>cat</i>, <i>run</i> and <i>of</i>, but <b>not</b> <i>elaborate</i> or <i>think</i>. Then the weights of <i>cat / run / of</i> will be added together to give the activation of KC36. </p>

<p>In the end, the IDs of the <i>n</i> Kenyon Cells with the highest activations are kept to produce a binary hash of the document. Again, <i>n</i> is a hyperparameter that can be tuned.</p>

  <ul class="Splitter">
    <li class="Splitter__Square Splitter__Square--small"></li>
    <li class="Splitter__Square Splitter__Square--big"></li>
    <li class="Splitter__Square Splitter__Square--small"></li>
  </ul>

     </section>
    </div>
    </div>
  </div>
  </body>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.0.2/jquery.min.js"></script>
  <script src="./scripts/bootstrap.min.js"></script>

</html>






