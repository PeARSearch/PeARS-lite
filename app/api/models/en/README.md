The current version of the 'enwikimodel' is trained:
- with a vocab size of 16K to try to get a better balance between search performance and speed;
- on wikipedia data `enwiki-20230901-pages-articles1.xml-p1p41242.5M` --> dump of Sept 1, 2023, 5 million words
